---
layout: post
title: "Bayesian Temporal Factorization for Multidimensional Time Series Prediction"
date: 2021-03-29 23:08:00
author: "Shawn"
header-style: text
catalog: true
tags:
  - MatrixFactorization
---

Given observed $Y$ and a trained model, one can first predict $\hat{x}_{t+1}$ on the latent temporal factor matrix $X$ and then estimate time series data at $t+1$ with $y_{i,t+1}\approx w_i^T\hat{x}_{t+1}$



## Bayesian Gaussian CP decomposition (BGCP)

### 1) Model Description

Given a matrix $\mathcal{Y}\in\mathbb{R}^{m\times n\times f}$ which suffers from missing values, then the factorization can be applied to reconstruct the missing values within $\mathcal{Y}$ by

$$
y_{ijt}=\sum_{s=1}^{r}u_{is} v_{js} x_{ts},\forall (i,j,t),
$$

where vectors $\boldsymbol{u}_{s}\in\mathbb{R}^{m},\boldsymbol{v}_{s}\in\mathbb{R}^{n},\boldsymbol{x}_{s}\in\mathbb{R}^{f}$ are columns of latent factor matrices, and $u_{is},v_{js},x_{ts}$ are their elements. The precision term $\tau$ is an inverse of Gaussian variance.

## Tucker Decomposition

### ALS Optimization

We could, for example, consider the following optimization problem for core tensor $\mathcal{G}\in\mathbb{R}^{R_1\times R_2\times R_3}$:


$$
\min_{\mathcal{G},U,V,X}\sum_{(i,j,t)\in\Omega}\left(y_{ijt}-\sum_{r_1=1}^{m}\sum_{r_2=1}^{n}\sum_{r_3=1}^{l}g_{r_1r_2r_3}u_{ir_1}v_{jr_2}x_{tr_3}\right)^2,
$$

$$
\Rightarrow\min_{\mathcal{G}}\sum_{(i,j,t)\in\Omega}\left(y_{ijt}-\left(\boldsymbol{x}_{t}\odot\boldsymbol{v}_{j}\odot\boldsymbol{u}_{i}\right)^\top\text{vec}\left(\mathcal{G}\right)\right)^2,
$$

$$
\Rightarrow\min_{\mathcal{G}}\sum_{(i,j,t)\in\Omega}\left(y_{ijt}-\left(\boldsymbol{x}_{t}\odot\boldsymbol{v}_{j}\odot\boldsymbol{u}_{i}\right)^\top\text{vec}\left(\mathcal{G}\right)\right)^\top\left(y_{ijt}-\left(\boldsymbol{x}_{t}\odot\boldsymbol{v}_{j}\odot\boldsymbol{u}_{i}\right)^\top\text{vec}\left(\mathcal{G}\right)\right),
$$

...

### Gradient Optimization

$$
\min J=\frac{1}{2} \sum_{(i, j, k) \in S} e_{i j k}^{2}=\frac{1}{2} \sum_{(i, j, k) \in S}\left(x_{i j k}-\sum_{m=1}^{r_{1}} \sum_{n=1}^{r_{2}} \sum_{l=1}^{r_{3}}\left(g_{m n l} \cdot u_{i m} \cdot v_{j n} \cdot w_{k l}\right)\right)^{2}
$$

$$
\frac{\partial J}{\partial u_{i m}}=-\sum_{j, k:(i, j, k) \in S} e_{i j k} \cdot\left(\sum_{n=1}^{r_{2}} \sum_{l=1}^{r_{3}}\left(g_{m n l} \cdot v_{j n} \cdot w_{k l}\right)\right)
$$

$$
\frac{\partial J}{\partial v_{j n}}=-\sum_{i, k:(i, j, k) \in S} e_{i j k} \cdot\left(\sum_{m=1}^{r_{1}} \sum_{l=1}^{r_{3}}\left(g_{m n l} \cdot u_{i m} \cdot w_{k l}\right)\right)
$$

$$
\frac{\partial J}{\partial w_{k l}}=-\sum_{i, j:(i, j, k) \in S} e_{i j k} \cdot\left(\sum_{m=1}^{r_{1}} \sum_{n=1}^{r_{2}}\left(g_{m n l} \cdot u_{i m} \cdot v_{j n}\right)\right)
$$

$$
\frac{\partial J}{\partial g_{m n l}}=-\sum_{(i, j, k) \in S} e_{i j k} \cdot u_{i m} \cdot v_{j n} \cdot w_{k l}
$$



### Long Sequence Transformer: Problems Reasons Solutions

Problem: Time & Space cost. $\rightarrow$ leading to a plethora of solutions such as parameter sharing, pruning, mixed-precision, and knowledge distillation.  

Recently, researchers have directly addressed the Transformer’s limitation by designing lower-complexity alternatives such as the **Longformer, Reformer, Linformer, and Performer.** However, due to the wide range of solutions, it has become challenging for the deep learning community to determine which methods to apply in practice to meet the desired trade-off between capacity, computation, and memory.

AttTF : An Attention-based Online Tensor Recovery Model

Variants: BAttTF

Missing data matrix $\rightarrow$ Find correlation $\rightarrow$ Factorization $\rightarrow$ Model

##### Q: How to update the correlation in matrix?

Using a constraint matrix factorization.

##### Q: How to compute&update attention?



##### Q：Why we need attention, a explicit&explainable reason?



> IS ATTENTION BETTER THAN MATRIX DECOMPOSITION? ICLR2021

这篇工作的主要重点是在于将网络中的Attention层/CNN/RNN层改为矩阵分解的模式进行学习。

> Deep Matrix Factorization Models for Recommender Systems

It measures the relevance between user and item by multi-layer non-linear projection and cosine similarity, it formulates as
$$
\begin{array}{l}
p_{i}=f_{\theta_{N}^{U}}\left(\ldots f_{\theta_{3}^{U}}\left(W_{U 2} f_{\theta_{2}^{U}}\left(Y_{i *} W_{U 1}\right)\right) \ldots\right) \\
q_{j}=f_{\theta_{N}^{I}}\left(\ldots f_{\theta_{3}^{I}}\left(W_{V 2} f_{\theta_{2}^{I}}\left(Y_{* j}^{T} W_{V 1}\right)\right) \ldots\right)
\end{array}
$$
with cosine similarity(as a fact, i think it mainly models as a cost function)
$$
\hat{Y}_{i j}=F^{D M F}\left(u_{i}, v_{j} \mid \Theta\right)=\operatorname{cosine}\left(p_{i}, q_{j}\right)=\frac{p_{i}^{T} q_{j}}{\left\|p_{i}\right\|\left\|q_{j}\right\|}
$$
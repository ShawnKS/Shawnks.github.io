---
layout: post
title: "Paper Note"
date: 2021-04-18 11:00:00
author: "Shawn"
header-style: text
catalog: true
tags:
  - matrix factorization
  - graph neural networks
  - sparse matrix storage
---

忙完了一阵，提交了两篇文章，下周还要提交一篇。有时候会后悔去年没投文章，不过可能这都是自己的选择。这个月还要改投一到两篇journal paper，关于NIPS和CIKM的实验也跑了一半，祝自己好运，anyway just follow your heart。

#### A New Approach for Sparse Matrix Classification Based on Deep Learning Techniques/Sparse Matrix Classification on Imbalanced Datasets Using Convolutional Neural Networks

A new methodology to **select the best storage format for sparse matrices based on deep learning techniques** is introduced. They focus on the selection of the proper format for the sparse matrix-vector multiplication(SpMV).

**Automatic selection of the best storage format for sparse matrices on GPUs.**

$\rightarrow$ 介绍许多压缩稀疏矩阵的格式，适合继续做图网络压缩！后面仔细读一下他的related works

#### PYTORCH-BIGGRAPH: A LARGE-SCALE GRAPH EMBEDDING SYSTEM

A little intuitive thinking: **Large Graph Neural Network Learning** $\rightarrow$ **Unsupervised Graph Embedding learning**

> Working with large graph data directly is difficlut, so a common technique is to use graph embedding methods to create vector representation for each node so that distances between these vectors predict the occurrence of edges in the graph.

#### HOW POWERFUL ARE GRAPH NEURAL NETWORKS?

We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the WeisfeilerLehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.

**Weisfeiler-Lehman (WL)**

#### From Graph Low-Rank Global Attention to 2-FWL Approximation

比较难读的一篇文章，作者用了比较多混淆却没有解释的公式。